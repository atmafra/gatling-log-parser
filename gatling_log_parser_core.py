import csv
import os
from datetime import datetime

import numpy as np


def parse_user_line(split: list) -> dict:
    """ Parses a single line of type USER from a Gatiling simulation log file

    :param split: line split tokens list
    :return: map of parsed field values
    """
    return {'type': split[0],
            'method': split[1],
            'request_id': int(split[2]),
            'event': split[3],
            'timestamp_start': int(split[4]),
            'timestamp_end': int(split[5])}


def parse_request_line(split: list) -> dict:
    """ Parses a single line of type REQUEST from a Gatiling simulation log file

    :param split: line split tokens list
    :return: map of parsed field values
    """
    return {'type': split[0],
            'request_id': int(split[1]),
            'endpoint': split[3],
            'timestamp_start': int(split[4]),
            'timestamp_end': int(split[5]),
            'response': split[6],
            'error_message': split[7]}


def parse_line(line: str) -> dict:
    """ Parses a single line of any from a Gatiling simulation log file

    :param line: single line of the simulation log file
    :return: map of parsed field values
    """
    split_line = line.split('\t')
    if len(split_line) > 0:
        line_type = split_line[0]
        if line_type == 'USER':
            return parse_user_line(split_line)
        elif line_type == 'REQUEST':
            return parse_request_line(split_line)
    return {}


def log_file_to_request_map(filepath: str) -> dict:
    """ Reads a Gatling simulation log file and generated an interpreted request map

    :param filepath: Gatling simulation log file path
    :return: interpreted request map
    """
    log_file = open(filepath, 'r')
    log_file.readline()  # header - unused
    request_map = {}
    for line in log_file:
        parsed_line = parse_line(line)
        request_id = parsed_line['request_id']
        line_type = parsed_line['type']

        if line_type == 'USER':
            event = parsed_line['event']
        else:
            event = 'RESPONSE'

        if request_id not in request_map:
            request_map[request_id] = {}
        request_map[request_id][event] = parsed_line

    return request_map


def _request_sort_key(request):
    return request['request_id']


def request_map_to_list(request_map: dict) -> list:
    """ Converts a request map produced by the interpretation of the log file into an ordered list of requests

    :param request_map: request map generated by log file parsing
    :return: sorted list (by request ID) of requests with its parsed attributes
    """
    request_list = []
    for request_id in request_map:
        # request fields
        request = {'request_id': request_id}
        request_entry = request_map[request_id]
        request_start = request_entry['START']
        request_end = request_entry['END']
        response = request_entry['RESPONSE']
        request['timestamp_request_start'] = request_end['timestamp_start']
        request['timestamp_request_end'] = request_end['timestamp_end']
        request['method'] = request_start['method']
        request['endpoint'] = response['endpoint']
        request['timestamp_response_start'] = response['timestamp_start']
        request['timestamp_response_end'] = response['timestamp_end']

        # calculated metrics
        request['request_latency'] = request['timestamp_response_start'] - request['timestamp_request_start']
        request['execution_time'] = request['timestamp_response_end'] - request['timestamp_response_start']
        request['response_latency'] = request['timestamp_request_end'] - request['timestamp_response_end']
        request['latency'] = request['request_latency'] + request['response_latency']
        request['response_time'] = request['timestamp_request_end'] - request['timestamp_request_start']

        request_list.append(request)

    request_list.sort(key=_request_sort_key)
    return request_list


def request_list_to_metrics_map(request_list: list) -> map:
    """ Converts the list of metrics associated to each request into a map of metrics lists

    :param request_list: list of requests with metrics
    :return: map of metrics lists (each dict entry maps to a list)
    """
    return {metric: np.array(list(request_list[i][metric] for i in range(len(request_list))))
            for metric in request_list[0].keys()}


def log_file_to_metrics_map(path: str, filename: str, verbose: bool = True) -> map:
    """ Reads a Gatling simulation log file and generated a map of metrics lists

    :param filepath: Gatling simulation log file path
    :param verbose: display execution messages in output terminal
    :return: map of metrics lists (each dict entry maps to a list)
    """
    log_filepath = os.path.join(path, filename)
    if verbose:
        print('Trying to parse Gatling log file \'{}\''.format(log_filepath))
    request_map = log_file_to_request_map(log_filepath)
    request_list = request_map_to_list(request_map)
    if verbose:
        print('{} requests parsed from log file'.format(len(request_list)))
    return request_list_to_metrics_map(request_list)


def get_time_parameters(timestamp_series: list) -> map:
    """ Gets the start and the end of a timestamp series as timestamps and datetimes

    :param timestamp_series: input timestamp series
    :return: start and end time parameters of the series
    """
    parameters = {'min_timestamp': np.min(timestamp_series), 'max_timestamp': np.max(timestamp_series)}
    parameters['min_datetime'] = datetime.fromtimestamp(parameters['min_timestamp'] / 1000.)
    parameters['max_datetime'] = datetime.fromtimestamp(parameters['max_timestamp'] / 1000.)
    return parameters


def get_test_time_buckets(timestamp_start: int,
                          timestamp_end: int,
                          bucket_width_ms: int = 1000) -> list:
    """ Generates a list of timestamp buckets by partitioning a time range into intervals of the same width

    :param timestamp_start: timestamp range start
    :param timestamp_end: timestamp range end
    :param bucket_width_ms: bucket width in milliseconds
    :return: list of time buckets
    """
    test_interval_ms = timestamp_end - timestamp_start
    num_buckets = test_interval_ms // bucket_width_ms
    if test_interval_ms % bucket_width_ms > 0:
        num_buckets += 1

    return list({'bucket_id': i,
                 'bucket_start_timestamp': timestamp_start + i * bucket_width_ms,
                 'bucket_end_timestamp': timestamp_start + (i + 1) * bucket_width_ms - 1,
                 'requests_started': [],
                 'requests_ended': [],
                 'responses_started': [],
                 'responses_ended': [],
                 'execution_time': [],
                 'request_latency': [],
                 'response_latency': [],
                 'latency': [],
                 'response_time': []} for i in range(num_buckets))


def timestamp_to_bucket_index(timestamp: int, timestamp_start: int, bucket_width_ms: int) -> int:
    """ Calculates the bucket index of the given timestamp

    :param timestamp: timestamp to be converted
    :param timestamp_start: start timestamp for reference
    :param bucket_width_ms: time bucket width in milliseconds
    :return: bucket index
    """
    return (timestamp - timestamp_start) // bucket_width_ms


def timestamp_to_bucket_list(timestamp_series: list, timestamp_start: int, bucket_width_ms: int) -> list:
    """ Converts a list of timestamps into a list of bucket indexed

    :param timestamp_series: input list of timestamps to be converted
    :param timestamp_start: start timestamp
    :param bucket_width_ms: time bucket width in milliseconds
    :return: the list of bucket indexes corresponding to each timestamp in the input list
    """
    return [timestamp_to_bucket_index(timestamp, timestamp_start, bucket_width_ms) for timestamp in timestamp_series]


def metrics_to_buckets(request_metrics: map, timings: list, timestamp_start: int, bucket_width_ms: int) -> map:
    """ Converts a set of timestamp series into a set of bucket lists

    :param request_metrics: a map containing the timestamp series and dictionary entries
    :param timings: map keys representing the timestamp series to be converted
    :param timestamp_start: start timestamp
    :param bucket_width_ms: time bucket width in milliseconds
    :return: a map of bucket index lists, resulting from the conversion of the timestamp lists (series)
    """
    return {timing: timestamp_to_bucket_list(request_metrics[timing], timestamp_start, bucket_width_ms)
            for timing in timings}


def get_statistic_parameters(metrics_list: list) -> map:
    """ Calculates a set of statistic parameters extracted from the given list of numeric metrics

    :param metrics_list: list of numeric metrics
    :return: map of statistic parameters and its values. All values will return zero if the list is empty.
        (min, max, average, stddev, median, top_90.00, top_99.00, top_99.90, top_99.99)
    """
    pos_9000 = round(0.9 * len(metrics_list)) - 1
    pos_9900 = round(0.99 * len(metrics_list)) - 1
    pos_9990 = round(0.999 * len(metrics_list)) - 1
    pos_9999 = round(0.9999 * len(metrics_list)) - 1
    if len(metrics_list) > 0:
        metrics_list_copy = metrics_list.copy()
        metrics_list_copy.sort()
        return {'min': metrics_list_copy[0],
                'max': metrics_list_copy[-1],
                'average': np.average(metrics_list_copy),
                'stddev': np.std(metrics_list_copy),
                'median': np.median(metrics_list_copy),
                'top_90.00': metrics_list_copy[pos_9000],
                'top_99.00': metrics_list_copy[pos_9900],
                'top_99.90': metrics_list_copy[pos_9990],
                'top_99.99': metrics_list_copy[pos_9999]}
    else:
        return {'min': 0,
                'max': 0,
                'average': 0,
                'stddev': 0,
                'median': 0,
                'top_90.00': 0,
                'top_99.00': 0,
                'top_99.90': 0,
                'top_99.99': 0}


def populate_time_buckets(buckets: list,
                          timings: list,
                          request_metrics: dict,
                          timestamp_start: int,
                          bucket_width_ms: int):
    """ Populates each bucket of a list of time buckets with its associated requests, responses, and metrics

    :param buckets: list of time buckets
    :param timings: timestamp series metric keys
    :param request_metrics: metrics to be used to populate the buckets
    :param timestamp_start: start timestamp
    :param bucket_width_ms: time bucket width in milliseconds
    """
    bucket_map = {timing: timestamp_to_bucket_list(request_metrics[timing], timestamp_start, bucket_width_ms)
                  for timing in timings}

    for i in range(len(request_metrics['request_id'])):
        request_start_bucket: dict = buckets[bucket_map['timestamp_request_start'][i]]
        request_end_bucket: dict = buckets[bucket_map['timestamp_request_end'][i]]
        response_start_bucket: dict = buckets[bucket_map['timestamp_response_start'][i]]
        response_end_bucket: dict = buckets[bucket_map['timestamp_response_end'][i]]

        request_start_bucket['requests_started'].append(request_metrics['request_id'][i])
        response_start_bucket['responses_started'].append(request_metrics['request_id'][i])

        request_end_bucket['requests_ended'].append(request_metrics['request_id'][i])
        request_end_bucket['request_latency'].append(request_metrics['request_latency'][i])
        request_end_bucket['latency'].append(request_metrics['latency'][i])
        request_end_bucket['response_time'].append(request_metrics['response_time'][i])

        response_end_bucket['responses_ended'].append(request_metrics['request_id'][i])
        response_end_bucket['execution_time'].append(request_metrics['execution_time'][i])
        response_end_bucket['response_latency'].append(request_metrics['response_latency'][i])


def request_map_to_time_buckets(request_metrics_map: dict, bucket_width_ms: int, verbose: bool = True):
    """ Creates a time bucket structure that is compatible with the request metrics map and populates it
        with request metrics from the metrics map

    :param request_metrics_map: map of lists of request metrics
    :param bucket_width_ms: bucket width in milliseconds
    :return: time bucket list structure populated with the associated requests, responses, and metrics
    """
    timings = ['timestamp_request_start',
               'timestamp_request_end',
               'timestamp_response_start',
               'timestamp_response_end']

    timestamp_series = {timing: get_time_parameters(request_metrics_map[timing]) for timing in timings}
    timestamp_start = timestamp_series['timestamp_request_start']['min_timestamp']
    timestamp_end = timestamp_series['timestamp_request_end']['max_timestamp']

    time_buckets = get_test_time_buckets(timestamp_start=timestamp_start,
                                         timestamp_end=timestamp_end,
                                         bucket_width_ms=bucket_width_ms)
    populate_time_buckets(buckets=time_buckets,
                          timings=timings,
                          request_metrics=request_metrics_map,
                          timestamp_start=timestamp_start,
                          bucket_width_ms=bucket_width_ms)

    if verbose:
        print('{} time buckets of {} milliseconds successfully populated'.format(len(time_buckets), bucket_width_ms))

    return time_buckets


def aggregate_time_buckets(time_buckets: list, bucket_width_ms: int, verbose: bool = True) -> dict:
    """ Calculates the statistic parameters for each list of metrics inside each time bucket of the given list

    :param time_buckets: list of time buckets
    :param bucket_width_ms: bucket width in milliseconds
    :return: a map of lists of statistic parameter values calculated for each time bucket
    """
    statistics = {'bucket_id': [],
                  'start_time_seconds': [],
                  'end_time_seconds': [],
                  'requests_started': [],
                  'requests_ended': [],
                  'responses_started': [],
                  'responses_ended': [],
                  'rps_started': [],
                  'rps_ended': []}

    metrics = ['request_latency', 'execution_time', 'response_latency', 'latency', 'response_time']

    for bucket in time_buckets:
        bucket_id = bucket['bucket_id']
        statistics['bucket_id'].append(bucket_id)
        statistics['start_time_seconds'].append(bucket_id * bucket_width_ms / 1000.)
        statistics['end_time_seconds'].append((bucket_id + 1) * bucket_width_ms / 1000.)
        statistics['requests_started'].append(len(bucket['requests_started']))
        statistics['requests_ended'].append(len(bucket['requests_ended']))
        statistics['responses_started'].append(len(bucket['responses_started']))
        statistics['responses_ended'].append(len(bucket['responses_ended']))
        statistics['rps_started'].append(len(bucket['requests_started']) * 1000. / bucket_width_ms)
        statistics['rps_ended'].append(len(bucket['responses_ended']) * 1000. / bucket_width_ms)

        for metric in metrics:
            parameters = get_statistic_parameters(bucket[metric])
            for parameter in parameters:
                key = metric + '_' + parameter
                if key not in statistics.keys():
                    statistics[key] = []
                statistics[key].append(parameters[parameter])

    if verbose:
        print('Bucket statistics successfully calculated')

    return statistics


def bucket_statistics_to_csv(statistics: map, path: str, filename: str, verbose: bool=True):
    """ Writes the bucket statistics dictionary to a CSV output file

    :param statistics: bucket statistics dictionary
    :param path: output .csv file path
    :param filename: output .csv file name
    :param verbose: display execution messages in the terminal output
    """
    csv_filepath = os.path.join(path, filename)
    with open(csv_filepath, 'w') as f:
        w = csv.writer(f)
        keys = list(statistics.keys())
        w.writerow(keys)
        bucket_count = len(statistics[keys[0]])
        for i in range(bucket_count):
            w.writerow(list([statistics[key][i] for key in keys]))
    f.close()

    if verbose:
        print('Successfully generated output file \'{}\''.format(csv_filepath))

    return
